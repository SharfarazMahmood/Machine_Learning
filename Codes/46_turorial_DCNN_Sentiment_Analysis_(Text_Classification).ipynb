{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"46_turorial_DCNN_Sentiment_Analysis_(Text_Classification).ipynb","provenance":[],"mount_file_id":"1Vp1nPXdRSv7mSwzWDo8aU47qrJ7PEiD3","authorship_tag":"ABX9TyPbbYvDctmcyJ77QAafwho4"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"XQvTRiSjPeAG"},"source":["https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/\n","\n","\n","/content/drive/My Drive/Work_FTFL_GIGATECH/FTFL_codes/txt_sentoken"]},{"cell_type":"code","metadata":{"id":"O0xLlhoBZirQ","executionInfo":{"status":"ok","timestamp":1604500265364,"user_tz":-360,"elapsed":1256,"user":{"displayName":"Sharfaraz Mahmood","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipagKbN03B6ZqQl9gO19YGUMX3i5aP71013KmlAQ=s64","userId":"05230969195319750445"}},"outputId":"890b7e7e-b19f-4787-f22d-72d189734583","colab":{"base_uri":"https://localhost:8080/"}},"source":["import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","\n","from string import punctuation\n","from os import listdir\n","from collections import Counter\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":41,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gimjZ7VHPKvE","executionInfo":{"status":"ok","timestamp":1604501121237,"user_tz":-360,"elapsed":556599,"user":{"displayName":"Sharfaraz Mahmood","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipagKbN03B6ZqQl9gO19YGUMX3i5aP71013KmlAQ=s64","userId":"05230969195319750445"}},"outputId":"7b14ec1e-a76e-41cc-c752-c6bb303b6563","colab":{"base_uri":"https://localhost:8080/"}},"source":["# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n"," \n","# turn a doc into clean tokens\n","def clean_doc(doc):\n","\t# split into tokens by white space\n","\ttokens = doc.split()\n","\t# remove punctuation from each token\n","\ttable = str.maketrans('', '', punctuation)\n","\ttokens = [w.translate(table) for w in tokens]\n","\t# remove remaining tokens that are not alphabetic\n","\ttokens = [word for word in tokens if word.isalpha()]\n","\t# filter out stop words\n","\tstop_words = set(stopwords.words('english'))\n","\ttokens = [w for w in tokens if not w in stop_words]\n","\t# filter out short tokens\n","\ttokens = [word for word in tokens if len(word) > 1]\n","\treturn tokens\n"," \n","# load doc and add to vocab\n","def add_doc_to_vocab(filename, vocab):\n","\t# load doc\n","\tdoc = load_doc(filename)\n","\t# clean doc\n","\ttokens = clean_doc(doc)\n","\t# update counts\n","\tvocab.update(tokens)\n"," \n","# load all docs in a directory\n","def process_docs(directory, vocab, is_trian):\n","\t# walk through all files in the folder\n","\tfor filename in listdir(directory):\n","\t\t# skip any reviews in the test set\n","\t\tif is_trian and filename.startswith('cv9'):\n","\t\t\tcontinue\n","\t\tif not is_trian and not filename.startswith('cv9'):\n","\t\t\tcontinue\n","\t\t# create the full path of the file to open\n","\t\tpath = directory + '/' + filename\n","\t\t# add doc to vocab\n","\t\tadd_doc_to_vocab(path, vocab)\n","\n","\n","# define vocab\n","vocab = Counter()\n","# add all docs to vocab\n","process_docs('/content/drive/My Drive/Work_FTFL_GIGATECH/FTFL_codes/txt_sentoken/neg', vocab, True)\n","process_docs('/content/drive/My Drive/Work_FTFL_GIGATECH/FTFL_codes/txt_sentoken/pos', vocab, True)\n","# print the size of the vocab\n","print(len(vocab))\n","# print the top words in the vocab\n","print(vocab.most_common(50))"],"execution_count":49,"outputs":[{"output_type":"stream","text":["41865\n","[('film', 7128), ('one', 4387), ('movie', 4123), ('like', 2823), ('even', 1983), ('good', 1853), ('time', 1823), ('story', 1703), ('films', 1700), ('much', 1616), ('also', 1608), ('would', 1605), ('characters', 1552), ('get', 1510), ('character', 1498), ('two', 1468), ('first', 1412), ('see', 1370), ('way', 1358), ('well', 1354), ('life', 1228), ('make', 1218), ('really', 1213), ('little', 1162), ('people', 1124), ('could', 1088), ('movies', 1086), ('scene', 1086), ('plot', 1077), ('best', 1067), ('never', 1056), ('many', 1032), ('man', 1030), ('new', 1029), ('bad', 1018), ('scenes', 989), ('doesnt', 950), ('great', 939), ('dont', 937), ('know', 919), ('hes', 903), ('another', 889), ('us', 883), ('love', 870), ('action', 853), ('still', 846), ('go', 842), ('seems', 835), ('something', 835), ('back', 827)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"S2xPf2xiarG2","executionInfo":{"status":"ok","timestamp":1604501122412,"user_tz":-360,"elapsed":1142,"user":{"displayName":"Sharfaraz Mahmood","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipagKbN03B6ZqQl9gO19YGUMX3i5aP71013KmlAQ=s64","userId":"05230969195319750445"}},"outputId":"4cc8a6a2-d9da-4150-defa-c30d39e83247","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(type(vocab))\n","backup_vocab = vocab.copy()\n","print(type(backup_vocab))\n","\n","# keep tokens with a min occurrence\n","min_occurane = 2\n","tokens = [k for k,c in vocab.items() if c >= min_occurane]\n","print(len(tokens))\n","\n","# save list to file\n","def save_list(lines, filename):\n","\t# convert lines to a single blob of text\n","\tdata = '\\n'.join(lines)\n","\t# open file\n","\tfile = open(filename, 'w')\n","\t# write text\n","\tfile.write(data)\n","\t# close file\n","\tfile.close()\n"," \n","# save tokens to a vocabulary file\n","save_list(tokens, 'vocab.txt')"],"execution_count":50,"outputs":[{"output_type":"stream","text":["<class 'collections.Counter'>\n","<class 'collections.Counter'>\n","24217\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0FOqvVyVdmiv","executionInfo":{"status":"ok","timestamp":1604501343878,"user_tz":-360,"elapsed":81172,"user":{"displayName":"Sharfaraz Mahmood","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipagKbN03B6ZqQl9gO19YGUMX3i5aP71013KmlAQ=s64","userId":"05230969195319750445"}},"outputId":"c417e719-70e7-4980-8dc1-3401cd879c13","colab":{"base_uri":"https://localhost:8080/"}},"source":["from numpy import array\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import Embedding\n","from keras.layers.convolutional import Conv1D\n","from keras.layers.convolutional import MaxPooling1D\n","\n","# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n"," \n","# turn a doc into clean tokens\n","def clean_doc(doc, vocab):\n","\t# split into tokens by white space\n","\ttokens = doc.split()\n","\t# remove punctuation from each token\n","\ttable = str.maketrans('', '', punctuation)\n","\ttokens = [w.translate(table) for w in tokens]\n","\t# filter out tokens not in vocab\n","\ttokens = [w for w in tokens if w in vocab]\n","\ttokens = ' '.join(tokens)\n","\treturn tokens\n"," \n","# load all docs in a directory\n","def process_docs(directory, vocab, is_trian):\n","\tdocuments = list()\n","\t# walk through all files in the folder\n","\tfor filename in listdir(directory):\n","\t\t# skip any reviews in the test set\n","\t\tif is_trian and filename.startswith('cv9'):\n","\t\t\tcontinue\n","\t\tif not is_trian and not filename.startswith('cv9'):\n","\t\t\tcontinue\n","\t\t# create the full path of the file to open\n","\t\tpath = directory + '/' + filename\n","\t\t# load the doc\n","\t\tdoc = load_doc(path)\n","\t\t# clean doc\n","\t\ttokens = clean_doc(doc, vocab)\n","\t\t# add to list\n","\t\tdocuments.append(tokens)\n","\treturn documents\n"," \n","# load the vocabulary\n","vocab_filename = 'vocab.txt'\n","vocab = load_doc(vocab_filename)\n","vocab = vocab.split()\n","vocab = set(vocab)\n","\n","# load all training reviews\n","positive_docs = process_docs('/content/drive/My Drive/Work_FTFL_GIGATECH/FTFL_codes/txt_sentoken/pos', vocab, True)\n","negative_docs = process_docs('/content/drive/My Drive/Work_FTFL_GIGATECH/FTFL_codes/txt_sentoken/neg', vocab, True)\n","train_docs = negative_docs + positive_docs\n","print(len(train_docs))\n","\n","# create the tokenizer\n","tokenizer = Tokenizer()\n","# fit the tokenizer on the documents\n","tokenizer.fit_on_texts(train_docs)\n","\n","# sequence encode\n","encoded_docs = tokenizer.texts_to_sequences(train_docs)\n","\n","# pad sequences\n","max_length = max([len(s.split()) for s in train_docs])\n","Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n","# define training labels\n","ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])\n","print(ytrain)"],"execution_count":51,"outputs":[{"output_type":"stream","text":["1800\n","[0 0 0 ... 1 1 1]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oKRA5nDSkjgp","executionInfo":{"status":"ok","timestamp":1604501426304,"user_tz":-360,"elapsed":82416,"user":{"displayName":"Sharfaraz Mahmood","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipagKbN03B6ZqQl9gO19YGUMX3i5aP71013KmlAQ=s64","userId":"05230969195319750445"}},"outputId":"9718e45a-986b-411e-eff4-e467b7c4d654","colab":{"base_uri":"https://localhost:8080/"}},"source":["# load all test reviews\n","positive_docs = process_docs('/content/drive/My Drive/Work_FTFL_GIGATECH/FTFL_codes/txt_sentoken/pos', vocab, False)\n","negative_docs = process_docs('/content/drive/My Drive/Work_FTFL_GIGATECH/FTFL_codes/txt_sentoken/neg', vocab, False)\n","test_docs = negative_docs + positive_docs\n","print(len(test_docs))\n","\n","# sequence encode\n","encoded_docs = tokenizer.texts_to_sequences(test_docs)\n","# pad sequences\n","Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n","# define test labels\n","ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])\n"," \n","# define vocabulary size (largest integer value)\n","vocab_size = len(tokenizer.word_index) + 1\n","print(vocab_size)\n"],"execution_count":52,"outputs":[{"output_type":"stream","text":["200\n","24218\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uFTB4Mqcp662","executionInfo":{"status":"ok","timestamp":1604501578517,"user_tz":-360,"elapsed":152193,"user":{"displayName":"Sharfaraz Mahmood","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipagKbN03B6ZqQl9gO19YGUMX3i5aP71013KmlAQ=s64","userId":"05230969195319750445"}},"outputId":"0b3088c6-5531-4eb9-f842-4b5a9560faeb","colab":{"base_uri":"https://localhost:8080/"}},"source":["# define model\n","model = Sequential()\n","model.add(Embedding(vocab_size, 100, input_length=max_length))\n","model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(Flatten())\n","model.add(Dense(10, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","print(model.summary())\n","\n","# compile network\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# fit network\n","model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n","\n","# evaluate\n","loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n","print('Test Accuracy: %f' % (acc*100))"],"execution_count":53,"outputs":[{"output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_3 (Embedding)      (None, 1313, 100)         2421800   \n","_________________________________________________________________\n","conv1d_3 (Conv1D)            (None, 1306, 32)          25632     \n","_________________________________________________________________\n","max_pooling1d_3 (MaxPooling1 (None, 653, 32)           0         \n","_________________________________________________________________\n","flatten_3 (Flatten)          (None, 20896)             0         \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 10)                208970    \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 1)                 11        \n","=================================================================\n","Total params: 2,656,413\n","Trainable params: 2,656,413\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","57/57 - 14s - loss: 0.6902 - accuracy: 0.5172\n","Epoch 2/10\n","57/57 - 14s - loss: 0.6363 - accuracy: 0.6361\n","Epoch 3/10\n","57/57 - 15s - loss: 0.3061 - accuracy: 0.8822\n","Epoch 4/10\n","57/57 - 14s - loss: 0.0497 - accuracy: 0.9800\n","Epoch 5/10\n","57/57 - 14s - loss: 0.0096 - accuracy: 0.9989\n","Epoch 6/10\n","57/57 - 14s - loss: 0.0029 - accuracy: 1.0000\n","Epoch 7/10\n","57/57 - 14s - loss: 0.0015 - accuracy: 1.0000\n","Epoch 8/10\n","57/57 - 15s - loss: 9.4540e-04 - accuracy: 1.0000\n","Epoch 9/10\n","57/57 - 16s - loss: 6.6490e-04 - accuracy: 1.0000\n","Epoch 10/10\n","57/57 - 15s - loss: 3.9220e-04 - accuracy: 1.0000\n","Test Accuracy: 82.499999\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"He1i1zlMrHtl"},"source":["##4. Train word2vec Embedding"]},{"cell_type":"code","metadata":{"id":"X68DcrwfrIyu"},"source":[""],"execution_count":null,"outputs":[]}]}